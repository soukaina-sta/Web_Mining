{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5LmbMF3C-4h"
   },
   "source": [
    "We will use \"IMDB movie review sentiment classification dataset\"\n",
    "\n",
    "Dataset Description: https://keras.io/api/datasets/imdb/\n",
    "\n",
    "This is a dataset of 25,000 movie reviews from IMDB, tagged by sentiment (positive/negative). The reviews have been preprocessed and each review is coded as a list of (whole) word indexes. For convenience, words are indexed by their overall frequency in the dataset, so that, for example, the integer \"3\" encodes the 3rd most frequent word in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jx_GPe7DC-4z"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "bijxRF3zC-47"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "numpy.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUCH3JAyC-5B"
   },
   "outputs": [],
   "source": [
    "db=imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2uiiEHXC-5D"
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_30gj1RC-5G",
    "outputId": "fa998306-4b09-4e7a-bb2f-55fcda0751c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdmbZdnHC-5O",
    "outputId": "04ef07b1-6eb5-4376-b34e-53c0a43211b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBKC8hJnC-5Q",
    "outputId": "4c269674-b5cc-432e-f7bf-a0ab598b3a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   19  178   32]\n",
      " [   0    0    0 ...   16  145   95]\n",
      " [   0    0    0 ...    7  129  113]\n",
      " ...\n",
      " [   0    0    0 ...    4 3586    2]\n",
      " [   0    0    0 ...   12    9   23]\n",
      " [   0    0    0 ...  204  131    9]]\n",
      "[[   0    0    0 ...   14    6  717]\n",
      " [   0    0    0 ...  125    4 3077]\n",
      " [  33    6   58 ...    9   57  975]\n",
      " ...\n",
      " [   0    0    0 ...   21  846    2]\n",
      " [   0    0    0 ... 2302    7  470]\n",
      " [   0    0    0 ...   34 2005 2643]]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "print(X_train)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN7-iv5BC-5T",
    "outputId": "94e0a075-417a-493a-8ce7-1617e7ef2127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNmmRHv6C-5U"
   },
   "source": [
    "we will use the embedding layer which defines the first hidden layer of the network. it must specify 3 arguments:\n",
    "\n",
    "input_dim: the size of the vocabulary in the text\n",
    "\n",
    "output_dim: this is the size of the vector space in which each word will be immersed\n",
    "\n",
    "input_legth: this is the size of the sequence, for example if your documents contain 100 words each then it is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vx5yWrl6C-5X",
    "outputId": "36130ed2-9914-4dd0-f660-9689684af106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " module_wrapper_3 (ModuleWra  (None, 500, 32)          160000    \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_4 (ModuleWra  (None, 500, 32)          3104      \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_5 (ModuleWra  (None, 250, 32)          0         \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - 380s 961ms/step - loss: 0.4437 - accuracy: 0.7813 - val_loss: 0.3096 - val_accuracy: 0.8704\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 229s 586ms/step - loss: 0.2405 - accuracy: 0.9061 - val_loss: 0.2810 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2518bf5ad00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.build(input_shape=(None, max_review_length))  # Build the model\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_3rdkZwC-5Z",
    "outputId": "6a6b213c-9876-4b63-9dcb-88c25dfb8fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.46%\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEUbxcGIC-5b"
   },
   "source": [
    "## simple example of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2S9E5cSC-5d"
   },
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QkCaeI_C-5f"
   },
   "outputs": [],
   "source": [
    "labels = [1,1,1,1,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nieSGEhKC-5g"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoSM7poVC-5h"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2R9PwhwC-5i",
    "outputId": "3326311c-b8ba-4b5b-8d37-1282bc9d9935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33, 49], [39, 34], [41, 43], [49, 34], [46], [21], [1, 43], [8, 39], [1, 34], [25, 8, 49, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8d2RQEOC-5k",
    "outputId": "c3139a24-292f-47ba-b473-7259941536da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33 49  0  0]\n",
      " [39 34  0  0]\n",
      " [41 43  0  0]\n",
      " [49 34  0  0]\n",
      " [46  0  0  0]\n",
      " [21  0  0  0]\n",
      " [ 1 43  0  0]\n",
      " [ 8 39  0  0]\n",
      " [ 1 34  0  0]\n",
      " [25  8 49  2]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmy_8YPAC-5l"
   },
   "source": [
    "We are now ready to define our Embedding layer as part of our model.\n",
    "\n",
    "The embedding has a vocabulary of 50 and an entry length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "\n",
    "The model is a simple binary classification model. It is important to note that the output of the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten it (the flatten layer) into a 32-element vector to pass it to the Dense output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2-P4uopC-5m",
    "outputId": "b810b870-445d-4d11-9b48-8cfa9c291686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " module_wrapper_6 (ModuleWra  (None, 4, 8)             400       \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Build the model\n",
    "model.build(input_shape=(None, max_length))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summarize the model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkIxL75uC-5p",
    "outputId": "8c66bd86-fc90-4c2e-98a1-b607870e3591"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251993876d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.array(labels)\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68B5UwMVC-5r",
    "outputId": "88214abc-b170-473c-8976-0561bc721f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuBD6faWC-5t"
   },
   "source": [
    "## To Do: \n",
    "\n",
    "1. Try the same thing on Google reviews dataset ( the file is given in the lab directory)\n",
    "2. try to change the embedding representation using Glove and Skipgram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxO1wY2CC-5u"
   },
   "source": [
    "###### Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "GbQAQS6TC-5w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqVQpHDzC-5y"
   },
   "source": [
    "###### Chargez les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "SOh-dyF7C-50",
    "outputId": "10cf622b-6f54-4bae-e63d-61369e9b433e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b2764920-4aaf-42c1-a7dd-ac7091f817ee\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>sortOrder</th>\n",
       "      <th>appId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrew Thomas</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
       "      <td>Update: After getting a response from the deve...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-05 22:25:57</td>\n",
       "      <td>According to our TOS, and the term you have ag...</td>\n",
       "      <td>2020-04-05 15:10:24</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Craig Haines</td>\n",
       "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
       "      <td>Used it for a fair amount of time without any ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-04 13:40:01</td>\n",
       "      <td>It sounds like you logged in with a different ...</td>\n",
       "      <td>2020-04-05 15:11:35</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steven adkins</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
       "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-01 16:18:13</td>\n",
       "      <td>This sounds odd! We are not aware of any issue...</td>\n",
       "      <td>2020-04-02 16:05:56</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lars Panzerbjørn</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
       "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-12 08:17:34</td>\n",
       "      <td>We do offer this option as part of the Advance...</td>\n",
       "      <td>2020-03-15 06:20:13</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scott Prewitt</td>\n",
       "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
       "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-14 17:41:01</td>\n",
       "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
       "      <td>2020-03-15 23:45:51</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2764920-4aaf-42c1-a7dd-ac7091f817ee')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b2764920-4aaf-42c1-a7dd-ac7091f817ee button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b2764920-4aaf-42c1-a7dd-ac7091f817ee');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           userName                                          userImage  \\\n",
       "0     Andrew Thomas  https://lh3.googleusercontent.com/a-/AOh14GiHd...   \n",
       "1      Craig Haines  https://lh3.googleusercontent.com/-hoe0kwSJgPQ...   \n",
       "2     steven adkins  https://lh3.googleusercontent.com/a-/AOh14GiXw...   \n",
       "3  Lars Panzerbjørn  https://lh3.googleusercontent.com/a-/AOh14Gg-h...   \n",
       "4     Scott Prewitt  https://lh3.googleusercontent.com/-K-X1-YsVd6U...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  Update: After getting a response from the deve...      1             21   \n",
       "1  Used it for a fair amount of time without any ...      1             11   \n",
       "2  Your app sucks now!!!!! Used to be good but no...      1             17   \n",
       "3  It seems OK, but very basic. Recurring tasks n...      1            192   \n",
       "4  Absolutely worthless. This app runs a prohibit...      1             42   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0             4.17.0.3  2020-04-05 22:25:57   \n",
       "1             4.17.0.3  2020-04-04 13:40:01   \n",
       "2             4.17.0.3  2020-04-01 16:18:13   \n",
       "3             4.17.0.2  2020-03-12 08:17:34   \n",
       "4             4.17.0.2  2020-03-14 17:41:01   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  According to our TOS, and the term you have ag...  2020-04-05 15:10:24   \n",
       "1  It sounds like you logged in with a different ...  2020-04-05 15:11:35   \n",
       "2  This sounds odd! We are not aware of any issue...  2020-04-02 16:05:56   \n",
       "3  We do offer this option as part of the Advance...  2020-03-15 06:20:13   \n",
       "4  We're sorry you feel this way! 90% of the app ...  2020-03-15 23:45:51   \n",
       "\n",
       "       sortOrder      appId  \n",
       "0  most_relevant  com.anydo  \n",
       "1  most_relevant  com.anydo  \n",
       "2  most_relevant  com.anydo  \n",
       "3  most_relevant  com.anydo  \n",
       "4  most_relevant  com.anydo  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('reviews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzg9dniTC-52"
   },
   "source": [
    "Nous avons choisi les colonnes \"score\" et \"content\" comme exemples pour les étiquettes de sentiment et les avis . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "uhfAFBT6C-53"
   },
   "outputs": [],
   "source": [
    "docs = data['content'].tolist()\n",
    "labels = data['score'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNuR4K6J27H-"
   },
   "source": [
    "1 = très négatif, 2 = négatif, 3 = neutre, 4 = positif, 5 = très positif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_TjA-98C-54",
    "outputId": "87a6b226-cfd5-4868-8eec-8a1bc9afa319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content  score\n",
      "0      Update: After getting a response from the deve...      1\n",
      "1      Used it for a fair amount of time without any ...      1\n",
      "2      Your app sucks now!!!!! Used to be good but no...      1\n",
      "3      It seems OK, but very basic. Recurring tasks n...      1\n",
      "4      Absolutely worthless. This app runs a prohibit...      1\n",
      "...                                                  ...    ...\n",
      "15741  I believe that this is by far the best app wit...      5\n",
      "15742                       It sometimes crashes a lot!!      5\n",
      "15743                         Works well for what I need      5\n",
      "15744                                           Love it.      5\n",
      "15745  Really amazing and helped me sooo much just i ...      5\n",
      "\n",
      "[15746 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "X = data['content']\n",
    "Y = data['score']\n",
    "df = pd.DataFrame({'content': X, 'score': Y})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfRNn5eRfTc"
   },
   "source": [
    "une opération de transformation sur les valeurs de la liste Y.\n",
    "\n",
    "Le but de cette transformation est de créer une nouvelle liste appelée labels qui représente une étiquette binaire pour chaque valeur de Y. Si la valeur dans Y est inférieure ou égale à 2, alors 0 est ajouté à labels, sinon 1 est ajouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNviIcLY3Jp7",
    "outputId": "29fc09b9-ab64-4263-e4e6-a8439f49d35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15746\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for value in Y.values:\n",
    "    if value <=2 :\n",
    "        labels.append(0)\n",
    "    else :\n",
    "        labels.append(1)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R59av1QBTAHj",
    "outputId": "d88c3ebd-1b78-4cf4-f084-84b15500d8db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Update: After getting a response from the deve...\n",
       "1        Used it for a fair amount of time without any ...\n",
       "2        Your app sucks now!!!!! Used to be good but no...\n",
       "3        It seems OK, but very basic. Recurring tasks n...\n",
       "4        Absolutely worthless. This app runs a prohibit...\n",
       "                               ...                        \n",
       "15741    I believe that this is by far the best app wit...\n",
       "15742                         It sometimes crashes a lot!!\n",
       "15743                           Works well for what I need\n",
       "15744                                             Love it.\n",
       "15745    Really amazing and helped me sooo much just i ...\n",
       "Name: content, Length: 15746, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqftfJIncdu9"
   },
   "source": [
    " # Prétraitement des données textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCimCqb8C-56",
    "outputId": "7fa1258f-a61c-4cd6-b6d8-6a635429fc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le vocab size :  11970\n",
      "Le max length: 393\n",
      "[[   0    0    0 ...    8   39  312]\n",
      " [   0    0    0 ...  151   40  546]\n",
      " [   0    0    0 ...    9   10 1591]\n",
      " ...\n",
      " [   0    0    0 ...   67    3   75]\n",
      " [   0    0    0 ...    0   51    4]\n",
      " [   0    0    0 ... 5449    9  390]]\n"
     ]
    }
   ],
   "source": [
    "# Préparer le tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)# Ajustement du tokenizer sur les textes X pour créer la représentation numérique des mots.\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Calcul de la taille du vocabulaire du tokenizer\n",
    "print(\"Le vocab size : \",vocab_size)\n",
    "\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(X)\n",
    "#print(encoded_docs)\n",
    "\n",
    "max_length = max([len(seq) for seq in encoded_docs])  # Calcul de la longueur maximale parmi toutes les séquences encodées\n",
    "print(\"Le max length:\",max_length)\n",
    "\n",
    "\n",
    "\n",
    "padded_docs1 = pad_sequences(encoded_docs, maxlen=max_length) # Rembourrage des séquences numériques pour avoir la même longueur\n",
    "print(padded_docs1)\n",
    "\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_docs1, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "by6_HSdn33LT",
    "outputId": "375f14c3-b812-420d-cb2e-d8673de9d1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 393, 150)          1795500   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 58950)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 58951     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,854,451\n",
      "Trainable params: 1,854,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 150, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Build the model\n",
    "model.build(input_shape=(None, max_length))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euH_qcyd6mDC",
    "outputId": "b8e714da-e3bc-47d0-8e3d-2eed6832081e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f725c651690>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now you can proceed with model training\n",
    "model.fit(padded_docs1, Y, epochs=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJp_FY1Y6qW9",
    "outputId": "71c8d22d-1c56-418f-87d1-f336f1be1b22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.161186\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs1, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9clPu5jC-6C"
   },
   "source": [
    "# Glove\n",
    "\n",
    "(Global Vectors for Word Representation) :\n",
    "\n",
    "GloVe est une méthode pour représenter les mots sous forme de vecteurs numériques appelés \"embeddings\".\n",
    "Ces embeddings capturent les relations sémantiques entre les mots, ce qui permet de mesurer la similarité et de faire des opérations vectorielles sur les mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPxMSdajYeZ7",
    "outputId": "98615a42-5433-4475-bac9-c61b5cbe39a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le vocab size :  11970\n",
      "Le max length: 393\n",
      "Padded Docs: [[   0    0    0 ...    8   39  312]\n",
      " [   0    0    0 ...  151   40  546]\n",
      " [   0    0    0 ...    9   10 1591]\n",
      " ...\n",
      " [   0    0    0 ...   67    3   75]\n",
      " [   0    0    0 ...    0   51    4]\n",
      " [   0    0    0 ... 5449    9  390]]\n"
     ]
    }
   ],
   "source": [
    "# Préparer le tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)# Ajustement du tokenizer sur les textes X pour créer la représentation numérique des mots.\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Calcul de la taille du vocabulaire du tokenizer\n",
    "print(\"Le vocab size : \",vocab_size)\n",
    "\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(X)\n",
    "#print(encoded_docs)\n",
    "\n",
    "max_length = max([len(seq) for seq in encoded_docs])  # Calcul de la longueur maximale parmi toutes les séquences encodées\n",
    "print(\"Le max length:\",max_length)\n",
    "\n",
    "\n",
    "\n",
    "padded_docs1 = pad_sequences(encoded_docs, maxlen=max_length) # Rembourrage des séquences numériques pour avoir la même longueur\n",
    "print(\"Padded Docs:\",padded_docs1)\n",
    "\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_docs1, labels, test_size=0.2, random_state=42)\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n",
    "val_X = np.array(val_X)\n",
    "val_Y = np.array(val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2Ezbcfbd0NV",
    "outputId": "a6946bc0-cf39-47ee-f580-be51b53992a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_42 (Embedding)    (None, 393, 100)          1197000   \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,314,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 1,197,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "394/394 [==============================] - 9s 18ms/step - loss: 0.5757 - accuracy: 0.6985 - val_loss: 0.5786 - val_accuracy: 0.6873\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.4985 - accuracy: 0.7448 - val_loss: 0.4661 - val_accuracy: 0.7787\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 6s 16ms/step - loss: 0.4551 - accuracy: 0.7764 - val_loss: 0.4539 - val_accuracy: 0.7832\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.4284 - accuracy: 0.7959 - val_loss: 0.4345 - val_accuracy: 0.7952\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 6s 16ms/step - loss: 0.3957 - accuracy: 0.8157 - val_loss: 0.4065 - val_accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.3473 - accuracy: 0.8421 - val_loss: 0.3890 - val_accuracy: 0.8257\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 7s 18ms/step - loss: 0.2878 - accuracy: 0.8777 - val_loss: 0.3716 - val_accuracy: 0.8403\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.2279 - accuracy: 0.9087 - val_loss: 0.3772 - val_accuracy: 0.8489\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 7s 18ms/step - loss: 0.1661 - accuracy: 0.9393 - val_loss: 0.3982 - val_accuracy: 0.8641\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 7s 17ms/step - loss: 0.1173 - accuracy: 0.9593 - val_loss: 0.3893 - val_accuracy: 0.8749\n",
      "99/99 [==============================] - 1s 8ms/step - loss: 0.3893 - accuracy: 0.8749\n",
      "Accuracy: 0.8749206066131592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    embedding_dim = len(first_line.split()) - 1  # Détermination de la dimension de l'embedding à partir de la première ligne du fichier GloVe\n",
    "print(embedding_dim)\n",
    "\n",
    "embeddings_index = {}  # Initialisation du dictionnaire pour stocker les embeddings GloVe\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # Extraction du mot à partir de la première valeur de chaque ligne\n",
    "        coefficients = np.asarray(values[1:], dtype='float32')  # Conversion des coefficients de l'embedding en un tableau numpy de type float32\n",
    "        embeddings_index[word] = coefficients  \n",
    "\n",
    "# Création d'une matrice d'embedding\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))  \n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)  # Récupération du vecteur d'embedding correspondant au mot dans le dictionnaire\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector  # Mise à jour de la ligne correspondante dans la matrice d'embedding\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))  # Ajout de la couche Embedding avec les embeddings pré-entrainés et non entraînables\n",
    "model.add(LSTM(128)) \n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "print(model.summary()) \n",
    "model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=10, batch_size=32)\n",
    "loss, accuracy = model.evaluate(val_X, val_Y)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAd5K7RgooIr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52qv8WYdYz_5"
   },
   "source": [
    "# SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7Vx8LnitTFJ"
   },
   "source": [
    "L'algorithme Skip-gram se concentre sur la prédiction du contexte à partir d'un mot cible. Plus précisément, il essaie de prédire les mots environnants (le contexte) d'un mot donné (le mot cible) dans un corpus de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D93VMyYriRL",
    "outputId": "27477c71-e1a9-4d77-ba60-8cb03ed46d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le vocab size :  22226\n",
      "Le max length: 391\n",
      "Padded Docs: [[    0     0     0 ...     7    32  1233]\n",
      " [    0     0     0 ...   147    35  3923]\n",
      " [    0     0     0 ...     9    10  8076]\n",
      " ...\n",
      " [    0     0     0 ...    64     3    75]\n",
      " [    0     0     0 ...     0    48    74]\n",
      " [    0     0     0 ... 15693     9   501]]\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_47 (Embedding)    (None, 391, 100)          2222600   \n",
      "                                                                 \n",
      " lstm_37 (LSTM)              (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,339,977\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 2,222,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "394/394 [==============================] - 11s 20ms/step - loss: 0.5484 - accuracy: 0.7126 - val_loss: 0.4939 - val_accuracy: 0.7451\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 7s 17ms/step - loss: 0.4884 - accuracy: 0.7570 - val_loss: 0.4629 - val_accuracy: 0.7794\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.4620 - accuracy: 0.7742 - val_loss: 0.4467 - val_accuracy: 0.7832\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 7s 17ms/step - loss: 0.4444 - accuracy: 0.7853 - val_loss: 0.4359 - val_accuracy: 0.7876\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.4205 - accuracy: 0.7979 - val_loss: 0.4262 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 7s 17ms/step - loss: 0.3960 - accuracy: 0.8118 - val_loss: 0.4441 - val_accuracy: 0.7984\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.3641 - accuracy: 0.8320 - val_loss: 0.4171 - val_accuracy: 0.8022\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 6s 16ms/step - loss: 0.3218 - accuracy: 0.8569 - val_loss: 0.3979 - val_accuracy: 0.8267\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 6s 15ms/step - loss: 0.2717 - accuracy: 0.8868 - val_loss: 0.3997 - val_accuracy: 0.8311\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 7s 17ms/step - loss: 0.5969 - accuracy: 0.7388 - val_loss: 0.4673 - val_accuracy: 0.7629\n",
      "99/99 [==============================] - 1s 7ms/step - loss: 0.4673 - accuracy: 0.7629\n",
      "Accuracy: 0.7628571391105652\n"
     ]
    }
   ],
   "source": [
    "# Préparer le tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Le vocab size : \", vocab_size)\n",
    "\n",
    "# Diviser chaque liste de mots en une autre liste de mots\n",
    "X = [[word for word in sentence] for sentence in X]\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(X)\n",
    "max_length = max([len(seq) for seq in encoded_docs])\n",
    "print(\"Le max length:\", max_length)\n",
    "\n",
    "padded_docs1 = pad_sequences(encoded_docs, maxlen=max_length)\n",
    "print(\"Padded Docs:\", padded_docs1)\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_docs1, labels, test_size=0.2, random_state=42)\n",
    "train_X = np.array(train_X)\n",
    "train_Y = np.array(train_Y)\n",
    "val_X = np.array(val_X)\n",
    "val_Y = np.array(val_Y)\n",
    "\n",
    "# Diviser chaque liste de mots en une autre liste de mots\n",
    "model = Word2Vec(sentences=X, sg=1, window=5, vector_size=100, epochs=10)\n",
    "\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# Création de la matrice d'embedding\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < vocab_size:\n",
    "        if word in model.wv:\n",
    "            embedding_vector = model.wv[word]\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=10, batch_size=32)\n",
    "loss, accuracy = model.evaluate(val_X, val_Y)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
